## DyME

### Empoweringâ€¯Smallâ€¯VLMsÂ ğŸ§ Â with Dynamicâ€¯MemorizationÂ &Â Exploration
> **â€œEmpoweringÂ SmallÂ VLMsÂ toâ€¯ThinkÂ withÂ Dynamicâ€¯Memorizationâ€¯andâ€¯Explorationâ€**  
> Jianâ€‘Liang Liuâ€¯*etâ€¯al.* [[arXivâ€¯2506.23061](https://arxiv.org/abs/2506.23061)]

[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![PythonÂ 3.10](https://img.shields.io/badge/python-3.10+-brightgreen.svg)](https://www.python.org/)
[![PyTorchÂ â‰¥Â 2.1](https://img.shields.io/badge/pytorch-â‰¥2.1-orange.svg)](https://pytorch.org/)
[![HuggingÂ Face](https://img.shields.io/badge/%F0%9F%A4%97-transformers-red.svg)](https://github.com/huggingface/transformers)

---

### âœ¨ Highlights
* **DynamicÂ MemoryÂ Module** â€“ differentiable read/write slots let small VLMs cache and reuse visual contexts.
* **Explorationâ€¯Controller** â€“ learns when to query memory vs. external knowledge for better reasoning.
* **Lightweight** â€“ fits on a single 8â€¯GB GPU for inference; full training needs one A100 or four 24â€¯GB GPUs.

---

## ğŸ“‚ Repository Layout
```

.
â”œâ”€â”€ src/       
â”œâ”€â”€ script/      
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## âš™ï¸ Installation
```bash
git clone 
cd DyME

# create env (conda recommended)
conda create -n DyME python=
conda activate DyME

# install dependencies
pip install -r requirements.txt
# or: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
````

---

## ğŸš€ Quick Start

### Inference

```python
from PIL import Image


```

### Finetuning

```bash
python 
```


---

## ğŸ”¬ Results



---

## ğŸ“– Citation

```bibtex
@article{liu2025empowering,
  title={Empowering Small VLMs to Think with Dynamic Memorization and Exploration},
  author={Liu, Jiazhen and Deng, Yuchuan and Chen, Long},
  journal={arXiv preprint arXiv:2506.23061},
  year={2025}
}
```

---

## ğŸ¤ Contributing

---

## ğŸ“ License

